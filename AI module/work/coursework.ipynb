{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow opencv-python mediapipe sklearn matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt # img show\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp_detection(image, model):\n",
    "   # recolours frame from default BGR -> RGB\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        # flips image so hands correct\n",
    "        image = cv2.flip(image,1)\n",
    "        # set flag\n",
    "        image.flags.writeable = False\n",
    "        # detect hands\n",
    "        results = model.process(image)\n",
    "        # sets flag back to true\n",
    "        image.flags.writeable = True\n",
    "        # recolours back to BGR\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "        return image,results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection(image, model):\n",
    "     # recolours frame from default BGR -> RGB\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        # flips image so hands correct\n",
    "        #image = cv2.flip(image,1)\n",
    "        # set flag\n",
    "        image.flags.writeable = False\n",
    "        # detect hands\n",
    "        results = model.process(image)\n",
    "        # sets flag back to true\n",
    "        image.flags.writeable = True\n",
    "        # recolours back to BGR\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR)\n",
    "        return image,results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    #Draws landmarks on image.\n",
    "    # hand landmark setup left-right\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks,\n",
    "    mp_holistic.HAND_CONNECTIONS,\n",
    "    mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=5),\n",
    "    mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=8))\n",
    "\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks,\n",
    "    mp_holistic.HAND_CONNECTIONS,\n",
    "    mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=5),\n",
    "    mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CaptureAndLandmark():\n",
    "    capture = cv2.VideoCapture(0) # get video from webcam\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "            while capture.isOpened():\n",
    "                    # read camera feed\n",
    "                    ret, frame = capture.read()\n",
    "\n",
    "                    # make detections\n",
    "                    image, results = detection(frame, holistic) # passing in the feed alongside the model\n",
    "\n",
    "                    # draw landmarks\n",
    "                    draw_landmarks(image,results)\n",
    "                    \n",
    "                    # display camera feed\n",
    "                    cv2.imshow('camera feed', image)\n",
    "\n",
    "                    \n",
    "                    # closes application\n",
    "                    if cv2.waitKey(25) & 0xFF == ord('r'): # key for closing webcam frame\n",
    "                            break\n",
    "\n",
    "            capture.release()\n",
    "            cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic feed and landmark display, no data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic feed and landmark display, no data collection.\n",
    "#CaptureAndLandmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keypoints for both hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoints(results):\n",
    "    # extracting the results from the feed into an array which contains the relative positioning of the left and right hand\n",
    "    # if hands are not present in the frame, populate an empty array with all zeros     \n",
    "    left = np.array([[res.x,res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    right = np.array([[res.x,res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    # numpy concatenation sequences the arrays into one array\n",
    "    return np.concatenate([left,right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the dimensions of the array, allows us to see if all data is correctly represented\n",
    "# each hand array should return 21 points per axis, so, for each hand 21*3 = 63\n",
    "# assuming all points are present, the shape of the array should be 126\n",
    "# keypoints(results).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data collection folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path for exported data\n",
    "path = os.path.join('data_set')\n",
    "# actions to be detected\n",
    "actions=np.array(['1 finger','2 fingers','3 fingers','4 fingers','5 fingers'])\n",
    "# number of videos\n",
    "no_sequences=30\n",
    "# length of frames\n",
    "sequence_length=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for act in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        # create a folder for each action, if the folder already exists then pass\n",
    "        try:\n",
    "            os.makedirs(os.path.join(path, act, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mz:\\University\\AI module\\work\\coursework.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/z%3A/University/AI%20module/work/coursework.ipynb#ch0000018?line=32'>33</a>\u001b[0m                         np\u001b[39m.\u001b[39msave(npy_path,keypoints)\n\u001b[0;32m     <a href='vscode-notebook-cell:/z%3A/University/AI%20module/work/coursework.ipynb#ch0000018?line=35'>36</a>\u001b[0m                         \u001b[39m# closes application\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/z%3A/University/AI%20module/work/coursework.ipynb#ch0000018?line=36'>37</a>\u001b[0m                         \u001b[39mif\u001b[39;00m cv2\u001b[39m.\u001b[39;49mwaitKey(\u001b[39m25\u001b[39;49m) \u001b[39m&\u001b[39m \u001b[39m0xFF\u001b[39m \u001b[39m==\u001b[39m \u001b[39mord\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m): \u001b[39m# key for closing webcam frame\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/z%3A/University/AI%20module/work/coursework.ipynb#ch0000018?line=37'>38</a>\u001b[0m                                 \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/z%3A/University/AI%20module/work/coursework.ipynb#ch0000018?line=39'>40</a>\u001b[0m capture\u001b[39m.\u001b[39mrelease()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "capture = cv2.VideoCapture(0) # get video from webcam\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        for act in actions:\n",
    "                for sequence in range(no_sequences):\n",
    "                        for frame_num in range(sequence_length):\n",
    "                                # read camera feed\n",
    "                                ret, frame = capture.read()\n",
    "\n",
    "                                # make detections\n",
    "                                image, results = detection(frame, holistic) # passing in the feed alongside the model\n",
    "\n",
    "                                # draw landmarks\n",
    "                                draw_landmarks(image,results)\n",
    "                                \n",
    "                                if frame_num == 0:\n",
    "                                        cv2.putText(image, 'Start', (120,200),\n",
    "                                         cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 1,cv2.LINE_AA)\n",
    "\n",
    "                                        cv2.putText(image, 'collecting for {} video number {}'.format(act, sequence),\n",
    "                                        (15,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),1,cv2.LINE_AA)\n",
    "                                        # display camera feed\n",
    "                                        cv2.imshow('camera feed', image)\n",
    "\n",
    "                                        cv2.waitKey(500)\n",
    "                                else:\n",
    "                                        cv2.putText(image, 'collecting for {} video number {}'.format(act, sequence),\n",
    "                                        (15,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),1,cv2.LINE_AA)\n",
    "                                        # display camera feed\n",
    "                                        cv2.imshow('camera feed', image)\n",
    "\n",
    "                                kp = keypoints(results)\n",
    "                                npy_path = os.path.join(path, act, str(sequence), str(frame_num))\n",
    "                                np.save(npy_path,keypoints)\n",
    "                                \n",
    "                               \n",
    "                                # closes application\n",
    "                                if cv2.waitKey(25) & 0xFF == ord('r'): # key for closing webcam frame\n",
    "                                        break\n",
    "\n",
    "        capture.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up a map for the recorded actions\n",
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load():\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return 0  # start with 0 if no storage present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(amount):\n",
    "    with open(path, \"wb\") as file:\n",
    "        pickle.dump(amount, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'data set'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mz:\\University\\AI module\\work\\coursework.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/University/AI%20module/work/coursework.ipynb#ch0000023?line=5'>6</a>\u001b[0m window \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/University/AI%20module/work/coursework.ipynb#ch0000023?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m frame_num \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(sequence_length):\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/University/AI%20module/work/coursework.ipynb#ch0000023?line=7'>8</a>\u001b[0m     \u001b[39m# load the data from the npy files stored in the data collection folders\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/z%3A/University/AI%20module/work/coursework.ipynb#ch0000023?line=8'>9</a>\u001b[0m     load()\n\u001b[0;32m     <a href='vscode-notebook-cell:/z%3A/University/AI%20module/work/coursework.ipynb#ch0000023?line=9'>10</a>\u001b[0m     save()\n\u001b[0;32m     <a href='vscode-notebook-cell:/z%3A/University/AI%20module/work/coursework.ipynb#ch0000023?line=10'>11</a>\u001b[0m     res \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, action, \u001b[39mstr\u001b[39m(sequence), \u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.npy\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(frame_num)), allow_pickle\u001b[39m=\u001b[39mtrue)\n",
      "\u001b[1;32mz:\\University\\AI module\\work\\coursework.ipynb Cell 22'\u001b[0m in \u001b[0;36mload\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/University/AI%20module/work/coursework.ipynb#ch0000021?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m():\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/University/AI%20module/work/coursework.ipynb#ch0000021?line=2'>3</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/z%3A/University/AI%20module/work/coursework.ipynb#ch0000021?line=3'>4</a>\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(path, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/University/AI%20module/work/coursework.ipynb#ch0000021?line=4'>5</a>\u001b[0m             \u001b[39mreturn\u001b[39;00m pickle\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m      <a href='vscode-notebook-cell:/z%3A/University/AI%20module/work/coursework.ipynb#ch0000021?line=5'>6</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'data set'"
     ]
    }
   ],
   "source": [
    "sequences, labels = [], []\n",
    "# loop through all of the actions, for each action, loop through all of the frames.\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        # creating a blank array to contain all the data in that particular sequence\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            # load the data from the npy files stored in the data collection folders\n",
    "            res = np.load(os.path.join(path, action, str(sequence), \"{}.npy\".format(frame_num)), allow_pickle=true)\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92d1c8824dd9ed58f2e2fe7d35df9188f2132fcf69312c535170a29d4fbf29a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
